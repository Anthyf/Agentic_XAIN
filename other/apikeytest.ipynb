{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b0cc42f",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c557d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "import openai\n",
    "import asyncio\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96d39be0-eaf3-456d-8613-ba21099ed36b",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "with open(\"../config/keys.yaml\") as f:\n",
    "    dict=yaml.safe_load(f)\n",
    "api_key = dict[\"API_keys\"][\"OpenAI\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caff6bf3",
   "metadata": {},
   "source": [
    "## Hello world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8858e54d",
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;01mawait\u001b[39;00m agent\u001b[38;5;241m.\u001b[39mrun(task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSay \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHello World!\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m model_client\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m (main())\n",
      "Cell \u001b[1;32mIn[17], line 2\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmain\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m----> 2\u001b[0m     model_client \u001b[38;5;241m=\u001b[39m OpenAIChatCompletionClient(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4o\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m     agent \u001b[38;5;241m=\u001b[39m AssistantAgent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, model_client\u001b[38;5;241m=\u001b[39mmodel_client)\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;01mawait\u001b[39;00m agent\u001b[38;5;241m.\u001b[39mrun(task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSay \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHello World!\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\yhe\\Desktop\\PHD\\Research\\25spring_Agentic_AI_project\\venv\\lib\\site-packages\\autogen_ext\\models\\openai\\_openai_client.py:1274\u001b[0m, in \u001b[0;36mOpenAIChatCompletionClient.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m   1271\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi_key\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m copied_args \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGEMINI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[0;32m   1272\u001b[0m         copied_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi_key\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGEMINI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m-> 1274\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[43m_openai_client_from_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopied_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1275\u001b[0m create_args \u001b[38;5;241m=\u001b[39m _create_args_from_config(copied_args)\n\u001b[0;32m   1277\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m   1278\u001b[0m     client\u001b[38;5;241m=\u001b[39mclient,\n\u001b[0;32m   1279\u001b[0m     create_args\u001b[38;5;241m=\u001b[39mcreate_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1282\u001b[0m     add_name_prefixes\u001b[38;5;241m=\u001b[39madd_name_prefixes,\n\u001b[0;32m   1283\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\yhe\\Desktop\\PHD\\Research\\25spring_Agentic_AI_project\\venv\\lib\\site-packages\\autogen_ext\\models\\openai\\_openai_client.py:111\u001b[0m, in \u001b[0;36m_openai_client_from_config\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_openai_client_from_config\u001b[39m(config: Mapping[\u001b[38;5;28mstr\u001b[39m, Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AsyncOpenAI:\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;66;03m# Shave down the config to just the OpenAI kwargs\u001b[39;00m\n\u001b[0;32m    110\u001b[0m     openai_config \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m openai_init_kwargs}\n\u001b[1;32m--> 111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m AsyncOpenAI(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mopenai_config)\n",
      "File \u001b[1;32mc:\\Users\\yhe\\Desktop\\PHD\\Research\\25spring_Agentic_AI_project\\venv\\lib\\site-packages\\openai\\_client.py:345\u001b[0m, in \u001b[0;36mAsyncOpenAI.__init__\u001b[1;34m(self, api_key, organization, project, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[0;32m    343\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 345\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[0;32m    346\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    347\u001b[0m     )\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m api_key\n\u001b[0;32m    350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "async def main() -> None:\n",
    "    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n",
    "    agent = AssistantAgent(\"assistant\", model_client=model_client)\n",
    "    print(await agent.run(task=\"Say 'Hello World!'\"))\n",
    "    await model_client.close()\n",
    "\n",
    "await (main())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dbd6b2",
   "metadata": {},
   "source": [
    "## Weather using api key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72b446bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- user ----------\n",
      "What is the weather in New York?\n",
      "---------- weather_agent ----------\n",
      "[FunctionCall(id='call_TVKc5PXSkeIUzzu6m9De1wux', arguments='{\"city\":\"New York\"}', name='get_weather')]\n",
      "---------- weather_agent ----------\n",
      "[FunctionExecutionResult(content='The weather in New York is 73 degrees and Sunny.', name='get_weather', call_id='call_TVKc5PXSkeIUzzu6m9De1wux', is_error=False)]\n",
      "---------- weather_agent ----------\n",
      "The weather in New York is currently 73 degrees and sunny.\n"
     ]
    }
   ],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "# Define a model client. You can use other model client that implements\n",
    "# the `ChatCompletionClient` interface.\n",
    "model_client = OpenAIChatCompletionClient(\n",
    "    model=\"gpt-4o\",\n",
    "    api_key=api_key,\n",
    ")\n",
    "\n",
    "# Define a simple function tool that the agent can use.\n",
    "# For this example, we use a fake weather tool for demonstration purposes.\n",
    "async def get_weather(city: str) -> str:\n",
    "    \"\"\"Get the weather for a given city.\"\"\"\n",
    "    return f\"The weather in {city} is 73 degrees and Sunny.\"\n",
    "\n",
    "\n",
    "# Define an AssistantAgent with the model, tool, system message, and reflection enabled.\n",
    "# The system message instructs the agent via natural language.\n",
    "agent = AssistantAgent(\n",
    "    name=\"weather_agent\",\n",
    "    model_client=model_client,\n",
    "    tools=[get_weather],\n",
    "    system_message=\"You are a helpful assistant.\",\n",
    "    reflect_on_tool_use=True,\n",
    "    model_client_stream=True,  # Enable streaming tokens from the model client.\n",
    ")\n",
    "\n",
    "\n",
    "# Run the agent and stream the messages to the console.\n",
    "async def main() -> None:\n",
    "    await Console(agent.run_stream(task=\"What is the weather in New York?\"))\n",
    "\n",
    "\n",
    "# NOTE: if running this inside a Python script you'll need to use asyncio.run(main()).\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0969e6bb",
   "metadata": {},
   "source": [
    "## Tell me a joke using api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75052d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'autogen'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mautogen\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magentchat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ConversableAgent\n\u001b[0;32m      6\u001b[0m agent \u001b[38;5;241m=\u001b[39m ConversableAgent(\n\u001b[0;32m      7\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchatbot\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      8\u001b[0m     llm_config\u001b[38;5;241m=\u001b[39mllm_config,\n\u001b[0;32m      9\u001b[0m     human_input_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNEVER\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m )\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'autogen'"
     ]
    }
   ],
   "source": [
    "from autogen.agentchat import ConversableAgent\n",
    "\n",
    "agent = ConversableAgent(\n",
    "    name=\"chatbot\",\n",
    "    llm_config=llm_config,\n",
    "    human_input_mode=\"NEVER\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727d9bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "reply = agent.generate_reply(\n",
    "    messages=[{\"content\": \"Tell me a joke.\", \"role\": \"user\"}]\n",
    ")\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1987f023",
   "metadata": {},
   "source": [
    "## Create a writer agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fe0f0a47-a9fe-43a0-b7b1-79922e4c4ac8",
   "metadata": {
    "height": 200
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "model_client = OpenAIChatCompletionClient(model=\"gpt-3.5-turbo\", api_key=api_key, seed=42, temperature=0)\n",
    "\n",
    "writer = AssistantAgent(\n",
    "    name=\"Writer\",\n",
    "    system_message=\"You are a writer. You write engaging and concise \" \n",
    "        \"blogpost (with title) on given topics. You must polish your \"\n",
    "        \"writing based on the feedback you receive and give a refined \"\n",
    "        \"version. Only return your final work without additional comments.\",\n",
    "    model_client=model_client,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b1b67d45",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ConversableAgent' from 'autogen_ext' (c:\\Users\\yhe\\Desktop\\PHD\\Research\\25spring_Agentic_AI_project\\venv\\lib\\site-packages\\autogen_ext\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mautogen_ext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ConversableAgent\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mdir\u001b[39m(ConversableAgent))\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'ConversableAgent' from 'autogen_ext' (c:\\Users\\yhe\\Desktop\\PHD\\Research\\25spring_Agentic_AI_project\\venv\\lib\\site-packages\\autogen_ext\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from autogen_ext import ConversableAgent\n",
    "print(dir(ConversableAgent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "093cc779",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ConversableAgent' from 'autogen_agentchat.agents' (c:\\Users\\yhe\\Desktop\\PHD\\Research\\25spring_Agentic_AI_project\\venv\\lib\\site-packages\\autogen_agentchat\\agents\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mautogen_agentchat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ConversableAgent\n\u001b[0;32m      2\u001b[0m model_client \u001b[38;5;241m=\u001b[39m OpenAIChatCompletionClient(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m, api_key\u001b[38;5;241m=\u001b[39mapi_key, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      4\u001b[0m agent \u001b[38;5;241m=\u001b[39m ConversableAgent(\n\u001b[0;32m      5\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchatbot\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m     model_client\u001b[38;5;241m=\u001b[39mmodel_client,\n\u001b[0;32m      7\u001b[0m     human_input_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNEVER\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      8\u001b[0m )\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'ConversableAgent' from 'autogen_agentchat.agents' (c:\\Users\\yhe\\Desktop\\PHD\\Research\\25spring_Agentic_AI_project\\venv\\lib\\site-packages\\autogen_agentchat\\agents\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from autogen_agentchat.agents import ConversableAgent\n",
    "model_client = OpenAIChatCompletionClient(model=\"gpt-3.5-turbo\", api_key=api_key, seed=42, temperature=0)\n",
    "\n",
    "agent = ConversableAgent(\n",
    "    name=\"chatbot\",\n",
    "    model_client=model_client,\n",
    "    human_input_mode=\"NEVER\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524f1ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "reply = agent.generate_reply(\n",
    "    messages=[{\"content\": \"Tell me a joke.\", \"role\": \"user\"}]\n",
    ")\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ca9fc86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__orig_bases__', '__parameters__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_add_messages_to_context', '_call_llm', '_check_and_handle_handoff', '_description', '_execute_tool_call', '_from_config', '_from_config_past_version', '_get_compatible_context', '_handoff_tools', '_handoffs', '_is_protocol', '_is_running', '_is_runtime_protocol', '_memory', '_model_client', '_model_client_stream', '_model_context', '_name', '_process_model_result', '_reflect_on_tool_use', '_reflect_on_tool_use_flow', '_summarize_tool_use', '_system_messages', '_to_config', '_tool_call_summary_format', '_tools', '_update_model_context_with_memory', 'close', 'component_config_schema', 'component_description', 'component_label', 'component_provider_override', 'component_type', 'component_version', 'description', 'dump_component', 'load_component', 'load_state', 'name', 'on_messages', 'on_messages_stream', 'on_pause', 'on_reset', 'on_resume', 'produced_message_types', 'required_class_vars', 'run', 'run_stream', 'save_state']\n"
     ]
    }
   ],
   "source": [
    "writer = AssistantAgent(name=\"Writer\",model_client=model_client)\n",
    "\n",
    "# List all available methods\n",
    "print(dir(writer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c7b4d8d-40f7-4a05-8958-25d20054de3a",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "AssistantAgent.on_messages() missing 1 required positional argument: 'cancellation_token'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m reply \u001b[38;5;241m=\u001b[39m \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: AssistantAgent.on_messages() missing 1 required positional argument: 'cancellation_token'"
     ]
    }
   ],
   "source": [
    "reply = writer.on_messages(messages=[{\"content\": task, \"role\": \"user\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c501c97d-e338-4f36-a384-6ec45983cf77",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "print(reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49658114",
   "metadata": {},
   "source": [
    "## Adding reflection \n",
    "\n",
    "Create a critic agent to reflect on the work of the writer agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fcd1c7-51ec-4915-8e97-bac03565c4c7",
   "metadata": {
    "height": 166
   },
   "outputs": [],
   "source": [
    "critic = autogen.AssistantAgent(\n",
    "    name=\"Critic\",\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\").find(\"TERMINATE\") >= 0,\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"You are a critic. You review the work of \"\n",
    "                \"the writer and provide constructive \"\n",
    "                \"feedback to help improve the quality of the content.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899d5fdb-6081-470b-b287-8cf8b8142d0d",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "res = critic.initiate_chat(\n",
    "    recipient=writer,\n",
    "    message=task,\n",
    "    max_turns=2,\n",
    "    summary_method=\"last_msg\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b76449",
   "metadata": {},
   "source": [
    "## Nested chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401ecf92-63e9-40ff-aeed-1c404352e4ab",
   "metadata": {
    "height": 200
   },
   "outputs": [],
   "source": [
    "SEO_reviewer = autogen.AssistantAgent(\n",
    "    name=\"SEO Reviewer\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"You are an SEO reviewer, known for \"\n",
    "        \"your ability to optimize content for search engines, \"\n",
    "        \"ensuring that it ranks well and attracts organic traffic. \" \n",
    "        \"Make sure your suggestion is concise (within 3 bullet points), \"\n",
    "        \"concrete and to the point. \"\n",
    "        \"Begin the review by stating your role.\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85acb81-7ab9-4c84-b8bb-6fbae3dce848",
   "metadata": {
    "height": 183
   },
   "outputs": [],
   "source": [
    "legal_reviewer = autogen.AssistantAgent(\n",
    "    name=\"Legal Reviewer\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"You are a legal reviewer, known for \"\n",
    "        \"your ability to ensure that content is legally compliant \"\n",
    "        \"and free from any potential legal issues. \"\n",
    "        \"Make sure your suggestion is concise (within 3 bullet points), \"\n",
    "        \"concrete and to the point. \"\n",
    "        \"Begin the review by stating your role.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46a177a-8088-4956-8d2b-3e916b8ca5e9",
   "metadata": {
    "height": 183
   },
   "outputs": [],
   "source": [
    "ethics_reviewer = autogen.AssistantAgent(\n",
    "    name=\"Ethics Reviewer\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"You are an ethics reviewer, known for \"\n",
    "        \"your ability to ensure that content is ethically sound \"\n",
    "        \"and free from any potential ethical issues. \" \n",
    "        \"Make sure your suggestion is concise (within 3 bullet points), \"\n",
    "        \"concrete and to the point. \"\n",
    "        \"Begin the review by stating your role. \",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b2ad6f-8ba6-436a-9459-14ffbe8a32d3",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "meta_reviewer = autogen.AssistantAgent(\n",
    "    name=\"Meta Reviewer\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"You are a meta reviewer, you aggragate and review \"\n",
    "    \"the work of other reviewers and give a final suggestion on the content.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913beca1",
   "metadata": {},
   "source": [
    "## Orchestrate the nested chats to solve the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11a70c7-19ca-4e5a-ad3d-f2b481fb5915",
   "metadata": {
    "height": 557
   },
   "outputs": [],
   "source": [
    "def reflection_message(recipient, messages, sender, config):\n",
    "    return f'''Review the following content. \n",
    "            \\n\\n {recipient.chat_messages_for_summary(sender)[-1]['content']}'''\n",
    "\n",
    "review_chats = [\n",
    "    {\n",
    "     \"recipient\": SEO_reviewer, \n",
    "     \"message\": reflection_message, \n",
    "     \"summary_method\": \"reflection_with_llm\",\n",
    "     \"summary_args\": {\"summary_prompt\" : \n",
    "        \"Return review into as JSON object only:\"\n",
    "        \"{'Reviewer': '', 'Review': ''}. Here Reviewer should be your role\",},\n",
    "     \"max_turns\": 1},\n",
    "    {\n",
    "    \"recipient\": legal_reviewer, \"message\": reflection_message, \n",
    "     \"summary_method\": \"reflection_with_llm\",\n",
    "     \"summary_args\": {\"summary_prompt\" : \n",
    "        \"Return review into as JSON object only:\"\n",
    "        \"{'Reviewer': '', 'Review': ''}.\",},\n",
    "     \"max_turns\": 1},\n",
    "    {\"recipient\": ethics_reviewer, \"message\": reflection_message, \n",
    "     \"summary_method\": \"reflection_with_llm\",\n",
    "     \"summary_args\": {\"summary_prompt\" : \n",
    "        \"Return review into as JSON object only:\"\n",
    "        \"{'reviewer': '', 'review': ''}\",},\n",
    "     \"max_turns\": 1},\n",
    "     {\"recipient\": meta_reviewer, \n",
    "      \"message\": \"Aggregrate feedback from all reviewers and give final suggestions on the writing.\", \n",
    "     \"max_turns\": 1},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a40b66-5061-460d-ad9d-c0dbcfbba2e9",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "critic.register_nested_chats(\n",
    "    review_chats,\n",
    "    trigger=writer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b8797d",
   "metadata": {},
   "source": [
    "**Note**: You might get a slightly different response than what's shown in the video. Feel free to try different task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8dcac3-1e72-43b7-9d5a-1be740f6efd5",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "res = critic.initiate_chat(\n",
    "    recipient=writer,\n",
    "    message=task,\n",
    "    max_turns=2,\n",
    "    summary_method=\"last_msg\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c833b0",
   "metadata": {},
   "source": [
    "## Get the summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ef82ed-f102-4964-b7be-60e2f258a39b",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "print(res.summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
