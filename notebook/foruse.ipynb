{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soley Narrator agent response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create Narrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED=42\n",
    "\n",
    "model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", api_key=api_key, seed=SEED, temperature=TEMPERATURE)\n",
    "\n",
    "narrator = AssistantAgent(\n",
    "    name=\"Narrator\",\n",
    "    system_message=\"You're a writer. Please generate your asnwer based on task and critic's feedback \",\n",
    "    model_client=model_client,\n",
    "    reflect_on_tool_use=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Soley Narrator response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def assistant_run() -> None:\n",
    "    response = await narrator.on_messages(\n",
    "        [TextMessage(content=prompt, source=\"user\")],\n",
    "        cancellation_token=CancellationToken(),\n",
    "    )\n",
    "    text=response.chat_message.content\n",
    "    #splitted_sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
    "    return [text] \n",
    "\n",
    "result_list = await assistant_run()\n",
    "#for sentence in result_list:\n",
    "    #print(sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using T's code to extract differences & I defined a function to summarize the errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract values from narrative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted story 1/1 with gpt-4o\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'Mjob_other': {'rank': 0,\n",
       "   'sign': -1,\n",
       "   'value': None,\n",
       "   'assumption': \"The model associates the mother's job being categorized as 'other' with less stable or lower-income jobs, potentially impacting the student's academic support at home.\"},\n",
       "  'failures': {'rank': 1,\n",
       "   'sign': 1,\n",
       "   'value': 0,\n",
       "   'assumption': 'A clean academic record, indicated by no past class failures, often correlates with future success.'},\n",
       "  'goout': {'rank': 2,\n",
       "   'sign': 1,\n",
       "   'value': None,\n",
       "   'assumption': 'A below-average tendency to go out with friends might indicate a more focused approach to studies.'},\n",
       "  'sex': {'rank': 3,\n",
       "   'sign': 1,\n",
       "   'value': 1,\n",
       "   'assumption': 'There is a pattern in the dataset where male students have a slightly higher pass rate.'}}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "extractor=ExtractionModel(ds_info=ds_info, llm=gpt)\n",
    "extraction=extractor.generate_extractions(Timmour_story)\n",
    "extraction\n",
    "#Because generate_extractions(...) returns a list of dicts—one dict per narrative—the 0th element is simply the dictionary for the first narrative. If you have multiple narratives, you’ll get multiple dictionaries in extraction, one for each entry. Accessing [0] is how you retrieve the very first dictionary in that list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2=extractor.generate_prompt(Timmour_story)\n",
    "print(prompt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_diff, sign_diff , value_diff, real_rank, extracted_rank=extractor.get_diff(extraction[0],generator.explanation_list[0])\n",
    "#compare the difference between generated SHAP table from GenerationModel (explanation_list) vs extracted features from ExtractionModel (extraction[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diff = pd.DataFrame({\n",
    "    \"rank\": rank_diff,\n",
    "    \"sign\": sign_diff,\n",
    "    \"value\": value_diff\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For row Mjob_other, column ['value'] didn't extract any value.\n",
      "For row goout, column ['value'] didn't extract any value.\n"
     ]
    }
   ],
   "source": [
    "def check_ones_and_none(df: pd.DataFrame,features_dict: dict) -> str:\n",
    "    \"\"\"\n",
    "    Checks each row of df for:\n",
    "      - Values not equal to 0 (treated as an error)\n",
    "      - Values that are None/NaN (didn't extract any value)\n",
    "\n",
    "    Integrates feature names from features_dict based on their 'rank'.\n",
    "    If no issues found, returns a '100.0% correct' message.\n",
    "    \"\"\"\n",
    "    # Build a mapping from row index (rank) to feature name\n",
    "    # Example: rank_to_feature[0] = 'failures'\n",
    "    rank_to_feature = {}\n",
    "    for feat_name, feat_info in features_dict.items():\n",
    "        rank = feat_info.get('rank')\n",
    "        if rank is not None:\n",
    "            rank_to_feature[rank] = feat_name\n",
    "\n",
    "    messages = []\n",
    "    found_issues = False\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        # Get the feature name for this row's rank\n",
    "        # If a rank is missing in the dictionary, default to \"Unknown\"\n",
    "        feature_name = rank_to_feature.get(idx, f\"Unknown_{idx}\")\n",
    "        \n",
    "        # Track columns with 1 and with None (NaN)\n",
    "        cols_with_one = []\n",
    "        cols_with_none = []\n",
    "        \n",
    "        for col, val in row.items():\n",
    "            if val != 0:\n",
    "                cols_with_one.append(col)\n",
    "            elif pd.isnull(val):\n",
    "                # This captures both None and NaN\n",
    "                cols_with_none.append(col)\n",
    "\n",
    "        # If we found any 1 in this row, report it\n",
    "        if cols_with_one:\n",
    "            found_issues = True\n",
    "            messages.append(f\"For row {feature_name}, column {cols_with_one} contains an error.\")\n",
    "        \n",
    "        # If we found any None/NaN in this row, report it\n",
    "        if cols_with_none:\n",
    "            found_issues = True\n",
    "            messages.append(f\"For row {feature_name}, column {cols_with_none} didn't extract any value.\")\n",
    "\n",
    "    # If we never found a 1 or a None/NaN, everything is 0\n",
    "    if not found_issues:\n",
    "        return \"After checking, the narrative is 100% correct.\"\n",
    "    \n",
    "    return \"\\n\".join(messages)\n",
    "\n",
    "feedback = check_ones_and_none(df_diff, extraction[0])\n",
    "print(feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main() -> None:\n",
    "\n",
    "    SEED=42\n",
    "    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", api_key=api_key, seed=SEED, temperature=TEMPERATURE)\n",
    "\n",
    "    narrator = AssistantAgent(\n",
    "        name=\"Narrator\",\n",
    "        system_message=\"You're a writer. Please generate your asnwer based on task and critic's feedback\",\n",
    "        model_client=model_client,\n",
    "        reflect_on_tool_use=False,\n",
    "    )\n",
    "\n",
    "    critic = AssistantAgent(\n",
    "        name=\"Critic\",\n",
    "        system_message=\"You are a critic. You review the content from\"\n",
    "                    \"the Narrator and provide constructive \"\n",
    "                    \"feedback to help improve the quality of the content.\",\n",
    "        model_client=model_client,\n",
    "        reflect_on_tool_use=False,\n",
    "    )\n",
    "    # The termination condition is a combination of text termination and max message termination, either of which will cause the chat to terminate.\n",
    "    termination = TextMentionTermination(\"TERMINATE\") | MaxMessageTermination(4)\n",
    "\n",
    "    summary_method=\"last_msg\"\n",
    "    # The group chat will alternate between the assistant and the code executor.\n",
    "    group_chat = RoundRobinGroupChat([narrator, critic], termination_condition=termination)\n",
    "\n",
    "    # `run_stream` returns an async generator to stream the intermediate messages.\n",
    "    stream = group_chat.run_stream(task=prompt)\n",
    "    \n",
    "    # `Console` is a simple UI to display the stream.\n",
    "    await Console(stream)\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define the order of groupchats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    allowed_or_disallowed_speaker_transitions={\n",
    "        user_proxy: [engineer, writer, executor, planner],\n",
    "        engineer: [user_proxy, executor],\n",
    "        writer: [user_proxy, planner],#Only specific agents are allowed to talk to certain others,\n",
    "        # only user_proxy and planner are alowed to response writer.\n",
    "        executor: [user_proxy, engineer, planner],\n",
    "        planner: [user_proxy, engineer, writer],\n",
    "    },\n",
    "    speaker_transitions_type=\"allowed\", "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nested group chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_agentchat.agents import AssistantAgent, SocietyOfMindAgent\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "from autogen_agentchat.conditions import TextMentionTermination\n",
    "\n",
    "\n",
    "async def main() -> None:\n",
    "    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", api_key=api_key, seed=SEED, temperature=TEMPERATURE)\n",
    "\n",
    "    agent1 = AssistantAgent(\"assistant1\", model_client=model_client, system_message=\"You are a writer, write well.\")\n",
    "    agent2 = AssistantAgent(\n",
    "        \"assistant2\",\n",
    "        model_client=model_client,\n",
    "        system_message=\"You are an editor, provide critical feedback. Respond with 'APPROVE' if the text addresses all feedbacks.\",\n",
    "    )\n",
    "    inner_termination = TextMentionTermination(\"APPROVE\")\n",
    "    inner_team = RoundRobinGroupChat([agent1, agent2], termination_condition=inner_termination)\n",
    "\n",
    "    society_of_mind_agent = SocietyOfMindAgent(\"society_of_mind\", team=inner_team, model_client=model_client)\n",
    "\n",
    "    agent3 = AssistantAgent(\n",
    "        \"assistant3\", model_client=model_client, system_message=\"Translate the text to Spanish.\"\n",
    "    )\n",
    "    team = RoundRobinGroupChat([society_of_mind_agent, agent3], max_turns=2)\n",
    "\n",
    "    stream = team.run_stream(task=\"Write a short story with a surprising ending.\")\n",
    "    await Console(stream)\n",
    "\n",
    "\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "Timmour_story=generator.generate_stories(trained_model,x,y)\n",
    "narrative_split=re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', Timmour_story[0])\n",
    "for sentence in narrative_split:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2= extractor.generate_prompt (Timmour_story)\n",
    "print(prompt2)\n",
    "# prompt2_split=re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', prompt2)\n",
    "# for sentence in prompt2_split:\n",
    "#     print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapnarrative_metrics.metrics import faithfulness\n",
    "test= faithfulness.average_zero(rank_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "with open(\"../config/keys.yaml\") as f:\n",
    "    dict=yaml.safe_load(f)\n",
    "api_key = dict[\"API_keys\"][\"OpenAI\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(\"../SHAPnarrative-metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use check ones and none to test if after manipulate, the function can completely identify where is wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPERATURE=0\n",
    "MANIP=True\n",
    "\n",
    "gpt = llm_wrappers.GptApi(api_key, model=\"gpt-4o\", system_role=\"You are a teacher that explains AI predictions.\", temperature=TEMPERATURE)\n",
    "generator=GenerationModel(ds_info=ds_info, llm=gpt)\n",
    "generator.gen_variables(trained_model,x,y,tree=True)\n",
    "shap_df=pd.DataFrame(generator.explanation_list[0].head(4))\n",
    "prompt = generator.generate_story_prompt(0,prompt_type=\"long\",manipulate=MANIP)\n",
    "print(prompt)\n",
    "\n",
    "Timmour_story=generator.generate_stories(trained_model,x,y,prompt_type=\"long\",manipulate=MANIP)\n",
    "extractor=ExtractionModel(ds_info=ds_info, llm=gpt)\n",
    "extraction=extractor.generate_extractions(Timmour_story)\n",
    "extraction\n",
    "\n",
    "def check_ones_and_none(df: pd.DataFrame,features_dict: dict) -> str:\n",
    "    \"\"\"\n",
    "    Checks each row of df for:\n",
    "      - Values not equal to 0 (treated as an error)\n",
    "      - Values that are None/NaN (didn't extract any value)\n",
    "\n",
    "    Integrates feature names from features_dict based on their 'rank'.\n",
    "    If no issues found, returns a '100.0% correct' message.\n",
    "    \"\"\"\n",
    "    # Build a mapping from row index (rank) to feature name\n",
    "    # Example: rank_to_feature[0] = 'failures'\n",
    "    rank_to_feature = {}\n",
    "    for feat_name, feat_info in features_dict.items():\n",
    "        rank = feat_info.get('rank')\n",
    "        if rank is not None:\n",
    "            rank_to_feature[rank] = feat_name\n",
    "\n",
    "    messages = []\n",
    "    found_issues = False\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        # Get the feature name for this row's rank\n",
    "        # If a rank is missing in the dictionary, default to \"Unknown\"\n",
    "        feature_name = rank_to_feature.get(idx, f\"Unknown_{idx}\")\n",
    "        \n",
    "        # Track columns with 1 and with None (NaN)\n",
    "        cols_with_one = []\n",
    "        cols_with_none = []\n",
    "        \n",
    "        for col, val in row.items():\n",
    "            if val != 0:\n",
    "                cols_with_one.append(col)\n",
    "            elif pd.isnull(val):\n",
    "                # This captures both None and NaN\n",
    "                cols_with_none.append(col)\n",
    "\n",
    "        # If we found any 1 in this row, report it\n",
    "        if cols_with_one:\n",
    "            found_issues = True\n",
    "            messages.append(f\"For row {feature_name}, column {cols_with_one} contains an error.\")\n",
    "        \n",
    "        # If we found any None/NaN in this row, report it\n",
    "        if cols_with_none:\n",
    "            found_issues = True\n",
    "            messages.append(f\"For row {feature_name}, column {cols_with_none} didn't extract any value.\")\n",
    "\n",
    "    # If we never found a 1 or a None/NaN, everything is 0\n",
    "    if not found_issues:\n",
    "        return \"After checking, the narrative is 100% correct.\"\n",
    "    \n",
    "    return \"\\n\".join(messages)\n",
    "\n",
    "feedback = check_ones_and_none(df_diff, extraction[0])\n",
    "print(feedback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Critic respond only with extracted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompt\n",
    "\n",
    "async def main() -> None:\n",
    "    SEED = 42\n",
    "    TEMPERATURE = 0.7  # Adjust as needed\n",
    "\n",
    "    # Assuming you have these imports/definitions\n",
    "    # from your_extraction_module import ExtractionModel, DatasetInfo\n",
    "    \n",
    "    # Initialize OpenAI client\n",
    "    model_client = OpenAIChatCompletionClient(\n",
    "        model=\"gpt-4o\", \n",
    "        api_key=api_key, \n",
    "        seed=SEED, \n",
    "        temperature=TEMPERATURE\n",
    "    )\n",
    "\n",
    "    # Custom extension of AssistantAgent to include extraction functionality\n",
    "    class ExtractorCriticAgent(AssistantAgent):\n",
    "        def __init__(self, *args, **kwargs):\n",
    "            super().__init__(*args, **kwargs)\n",
    "            self.extractor = None\n",
    "            self.last_narrative = None\n",
    "\n",
    "        def set_extractor(self, extractor_class, ds_info, llm):\n",
    "            \"\"\"\n",
    "            Set up the extraction model\n",
    "            \n",
    "            Args:\n",
    "                extractor_class: The ExtractionModel class\n",
    "                ds_info: Dataset information\n",
    "                llm: Language model client\n",
    "            \"\"\"\n",
    "            self.extractor = extractor_class(ds_info=ds_info, llm=llm)\n",
    "\n",
    "        def prepare_critic_input(self, narrative):\n",
    "            \"\"\"\n",
    "            Generate extractions from the narrative\n",
    "            \n",
    "            Args:\n",
    "                narrative (str): The narrative text to extract from\n",
    "            \n",
    "            Returns:\n",
    "                dict: Extracted information\n",
    "            \"\"\"\n",
    "            if not self.extractor:\n",
    "                raise ValueError(\"Extractor not initialized. Call set_extractor first.\")\n",
    "            \n",
    "            # Store the narrative for potential future use\n",
    "            self.last_narrative = narrative\n",
    "            \n",
    "            # Generate extractions\n",
    "            try:\n",
    "                extraction = self.extractor.generate_extractions(narrative)\n",
    "                return extraction\n",
    "            except Exception as e:\n",
    "                print(f\"Extraction error: {e}\")\n",
    "                return {\"error\": str(e)}\n",
    "\n",
    "    # Create the narrator agent\n",
    "    narrator = AssistantAgent(\n",
    "        name=\"Narrator\", \n",
    "        system_message=system_message_narrator, \n",
    "        model_client=model_client,\n",
    "        reflect_on_tool_use=False,\n",
    "    )\n",
    "\n",
    "    # Create the critic agent with extraction capabilities\n",
    "    critic = ExtractorCriticAgent(\n",
    "        name=\"Critic\", \n",
    "        system_message=system_message_critic, \n",
    "        model_client=model_client,\n",
    "        reflect_on_tool_use=False,\n",
    "    )\n",
    "\n",
    "    # Set up the extractor for the critic\n",
    "    critic.set_extractor(ExtractionModel, ds_info, gpt)\n",
    "\n",
    "    # Modify the group chat to pass narrative to extraction before critic review\n",
    "    class CustomGroupChat(RoundRobinGroupChat):\n",
    "        async def _process_message(self, sender, recipient):\n",
    "            # If moving from narrator to critic, extract information first\n",
    "            if sender.name == \"Narrator\" and recipient.name == \"Critic\":\n",
    "                # Get the last message from the narrator\n",
    "                last_message = sender.last_message()\n",
    "                \n",
    "                if last_message and 'content' in last_message:\n",
    "                    # Prepare extraction for the critic\n",
    "                    extraction = recipient.prepare_critic_input(last_message['content'])\n",
    "                    \n",
    "                    # Modify the message to include extraction\n",
    "                    last_message['content'] += f\"\\n\\nExtracted Information:\\n{extraction}\"\n",
    "            \n",
    "            # Continue with standard group chat processing\n",
    "            return await super()._process_message(sender, recipient)\n",
    "\n",
    "    # Create the group chat with custom processing\n",
    "    termination = TextMentionTermination(\"TERMINATE\") | MaxMessageTermination(8)\n",
    "    group_chat = CustomGroupChat(\n",
    "        [narrator, critic], \n",
    "        termination_condition=termination\n",
    "    )\n",
    "\n",
    "    # Run the chat\n",
    "    stream = group_chat.run_stream(task=prompt)\n",
    "    await Console(stream)\n",
    "\n",
    "# Run the main async function\n",
    "if __name__ == \"__main__\":\n",
    "  await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate if no data leakage and each round the comparison is between orginal shap_df and extracted information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "story='''The model predicted with 64% certainty that the student would pass the final year. The most significant factor contributing to this prediction was the father's education level, which, despite being below the average, had a positive impact. This suggests that even a primary education level can provide some foundational support for the student's academic journey. The student receiving extra educational support also played a crucial role in boosting the likelihood of passing, as it provided additional resources and guidance. The student's history of one past class failure, which is above the average, negatively impacted the prediction, indicating potential academic challenges. Lastly, the student's age of 19, while above the average, contributed positively, possibly reflecting a level of maturity and focus beneficial for academic success. Overall, the positive influences of the father's education, extra support, and maturity outweighed the negative impact of past failures, leading to the model's prediction that the student would pass.\n",
    "'''\n",
    "\n",
    "import re\n",
    "narrative_split=re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', story)\n",
    "for sentence in narrative_split:\n",
    "    print(sentence)\n",
    "\n",
    "extractor=ExtractionModel(ds_info=ds_info, llm=gpt)\n",
    "extraction=extractor.generate_extractions([story])\n",
    "extraction\n",
    "# extraction_string=extractor.generate_prompt(story)\n",
    "# print(extraction_string)\n",
    "\n",
    "rank_diff, sign_diff , value_diff, real_rank, extracted_rank=extractor.get_diff(extraction[0],shap_df)\n",
    "#compare the difference between generated SHAP table from GenerationModel (explanation_list) vs extracted features from ExtractionModel (extraction[0])'''\n",
    "\n",
    "df_diff = pd.DataFrame({\n",
    "    \"rank\": rank_diff,\n",
    "    \"sign\": sign_diff,\n",
    "    \"value\": value_diff\n",
    "})\n",
    "print(df_diff)\n",
    "\n",
    "critic = ExtractorCriticAgent(\n",
    "        name=\"Critic\",\n",
    "        system_message=\"\"\"\n",
    "You are a validation expert for SHAP narrative extraction analysis. \n",
    "Your feedback should focus ONLY on the generated validation_feedback.\n",
    "Do not provide general comments or suggestions not directly based on the validation_feedback.\n",
    "\"\"\",\n",
    "        description=\"An agent that extracts and validates narrative faithfulness\",\n",
    "    )\n",
    "    \n",
    "    # Set up the models for the critic - THIS IS IMPORTANT\n",
    "critic.set_models(\n",
    "        extractor_class=ExtractionModel,\n",
    "        generator_class=GenerationModel,\n",
    "        ds_info=ds_info,\n",
    "        llm=gpt\n",
    "    )\n",
    "\n",
    "errors= critic.prepare_critic_input([story],shap_df)\n",
    "print (errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two agent system with defined well-structured format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_df = shap_df\n",
    "\n",
    "async def main() -> None:\n",
    "    SEED = 42\n",
    "    TEMPERATURE = 0\n",
    "\n",
    "    # Initialize OpenAI client\n",
    "    model_client = OpenAIChatCompletionClient(\n",
    "        model=\"gpt-4o\",\n",
    "        api_key=api_key,\n",
    "        seed=SEED,\n",
    "        temperature=TEMPERATURE\n",
    "    )\n",
    "    \n",
    "    # Create the narrator agent\n",
    "    narrator = NarratorAgent(\n",
    "        name=\"narrator\",\n",
    "        system_message=system_message_narrator,\n",
    "        model_client=model_client,\n",
    "        reflect_on_tool_use=False,\n",
    "    )\n",
    "    \n",
    "    # Create the critic agent with advanced extraction capabilities\n",
    "    extractorcritic = ExtractorCriticAgent(\n",
    "        name=\"extractorcritic\",\n",
    "    )\n",
    "    \n",
    "    # Set up the models for the critic - THIS IS IMPORTANT\n",
    "    extractorcritic.set_models(\n",
    "        extractor_class=ExtractionModel,\n",
    "        generator_class=GenerationModel,\n",
    "        ds_info=ds_info,\n",
    "        llm=gpt\n",
    "    )\n",
    "\n",
    "    extractorcritic.default_shap_df = shap_df \n",
    "\n",
    "    # Create the group chat with custom processing\n",
    "    termination = TextMentionTermination(\"TERMINATE\") | MaxMessageTermination(12)\n",
    "    \n",
    "    group_chat = RoundRobinGroupChat(\n",
    "        [narrator, extractorcritic],\n",
    "        termination_condition=termination\n",
    "    )\n",
    "    \n",
    "    # Run the chat\n",
    "    stream = group_chat.run_stream(task=prompt)\n",
    "    \n",
    "    await Console(stream)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Three agent system with extracor critic and faithful critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_df = shap_df\n",
    "\n",
    "async def main() -> None:\n",
    "    SEED = 42\n",
    "    TEMPERATURE = 0\n",
    "\n",
    "    # Initialize OpenAI client\n",
    "    model_client = OpenAIChatCompletionClient(\n",
    "        model=\"gpt-4o\",\n",
    "        api_key=api_key,\n",
    "        seed=SEED,\n",
    "        temperature=TEMPERATURE\n",
    "    )\n",
    "    \n",
    "    # Create the narrator agent\n",
    "    narrator = NarratorAgent(\n",
    "        name=\"narrator\",\n",
    "        system_message=narrator_sys_msg, \n",
    "        model_client=model_client,\n",
    "        reflect_on_tool_use=False,\n",
    "    )\n",
    "    \n",
    "    # Create the critic agent with advanced extraction capabilities\n",
    "    faithful_evaluator = FaithfulEvaluatorAgent(\n",
    "        name=\"faithful_evaluator\",\n",
    "    )\n",
    "    \n",
    "    # Set up the models for the critic - THIS IS IMPORTANT\n",
    "    faithful_evaluator.set_models(\n",
    "        extractor_class=ExtractionModel,\n",
    "        generator_class=GenerationModel,\n",
    "        ds_info=ds_info,\n",
    "        llm=gpt\n",
    "    )\n",
    "\n",
    "    faithful_evaluator.default_shap_df = shap_df \n",
    "\n",
    "    faithful_critic = AssistantAgent(\n",
    "        name=\"faithful_critic\",\n",
    "        system_message=faithful_critic_sys_msg,\n",
    "        model_client=model_client,\n",
    "        reflect_on_tool_use=False,\n",
    "    )\n",
    "    # Create the group chat with custom processing\n",
    "    termination = TextMentionTermination(\"TERMINATE\") | MaxMessageTermination(20)\n",
    "    \n",
    "    group_chat = RoundRobinGroupChat(\n",
    "        [narrator, faithful_evaluator, faithful_critic],\n",
    "        model_client=model_client,\n",
    "        termination_condition=termination,\n",
    "    )\n",
    "\n",
    "    # Run the chat\n",
    "    await Console(group_chat.run_stream(task=prompt))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Four agent system with the coherence agent without any function call or perplexity.\n",
    "After reaching 100% faithulness, the new perplexity start to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.teams import SelectorGroupChat\n",
    "from autogen_agentchat.messages import AgentEvent, ChatMessage\n",
    "from selector_func import selector_func\n",
    "\n",
    "import re\n",
    "\n",
    "faithfulness_reached = False\n",
    "coherence_phase_started = False\n",
    "\n",
    "shap_df = shap_df\n",
    "\n",
    "async def main() -> None:\n",
    "    SEED = 42\n",
    "    TEMPERATURE = 0\n",
    "\n",
    "    # Initialize OpenAI client\n",
    "    model_client = OpenAIChatCompletionClient(\n",
    "        model=\"gpt-4o\",\n",
    "        api_key=api_key,\n",
    "        seed=SEED,\n",
    "        temperature=TEMPERATURE\n",
    "    )\n",
    "    \n",
    "    # Create the narrator agent\n",
    "    narrator = NarratorAgent(\n",
    "        name=\"narrator\",\n",
    "        system_message=narrator_sys_msg, \n",
    "        model_client=model_client,\n",
    "        reflect_on_tool_use=False,\n",
    "    )\n",
    "    \n",
    "    # Create the critic agent with advanced extraction capabilities\n",
    "    faithful_evaluator = FaithfulEvaluatorAgent(\n",
    "        name=\"faithful_evaluator\",\n",
    "    )\n",
    "    \n",
    "    # Set up the models for the critic - THIS IS IMPORTANT\n",
    "    faithful_evaluator.set_models(\n",
    "        extractor_class=ExtractionModel,\n",
    "        generator_class=GenerationModel,\n",
    "        ds_info=ds_info,\n",
    "        llm=gpt\n",
    "    )\n",
    "\n",
    "    faithful_evaluator.default_shap_df = shap_df \n",
    "\n",
    "    faithful_critic = AssistantAgent(\n",
    "        name=\"faithful_critic\",\n",
    "        system_message=faithful_critic_sys_msg,\n",
    "        model_client=model_client,\n",
    "        reflect_on_tool_use=False,\n",
    "    )\n",
    "\n",
    "    coherenceagent = AssistantAgent(\n",
    "            name=\"coherenceagent\",\n",
    "            system_message=coherence_sys_msg, \n",
    "            model_client=model_client,\n",
    "            reflect_on_tool_use=False,\n",
    "        )\n",
    "\n",
    "    # Create the group chat with custom processing\n",
    "    termination = TextMentionTermination(\"TERMINATE\") | MaxMessageTermination(20)\n",
    "    \n",
    "    group_chat = SelectorGroupChat(\n",
    "        [narrator, faithful_evaluator, faithful_critic, coherenceagent],\n",
    "        model_client=model_client,\n",
    "        selector_func=selector_func,\n",
    "        termination_condition=termination,\n",
    "    )\n",
    "\n",
    "    # Run the chat\n",
    "    await Console(group_chat.run_stream(task=prompt))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    await main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
